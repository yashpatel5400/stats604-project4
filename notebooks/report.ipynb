{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if \"data\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # ignore FutureWarnings from pd\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import urllib.request\n",
    "import json\n",
    "import requests\n",
    "from math import ceil\n",
    "# import multiprocessing\n",
    "# from multiprocess.pool import Pool\n",
    "\n",
    "from raw_data import wunderground_download\n",
    "import predictor.utils as utils\n",
    "from predictor.models.predictor_zeros import ZerosPredictor\n",
    "from predictor.models.unique import ArimaPredictor\n",
    "from predictor.models.unique import HistoricAveragePredictor\n",
    "from predictor.models.seamus import BasicOLSPredictor\n",
    "from predictor.models.seamus import LassoPredictor\n",
    "from predictor.models.seamus import GBTPredictor\n",
    "from predictor.models.vinod import PrevDayHistoricalPredictor\n",
    "from predictor.models.vinod import MetaPredictor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from predictor.models.predictor_scaffold import Predictor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "keep_features = ['temp_min', 'wspd_min', 'pressure_min', 'heat_index_min', 'dewPt_min',\n",
    "   'temp_mean', 'wspd_mean', 'pressure_mean', 'heat_index_mean',\n",
    "   'dewPt_mean', 'temp_max', 'wspd_max', 'pressure_max', 'heat_index_max',\n",
    "   'dewPt_max', 'wdir_mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Prediction\n",
    "By Yash Patel, Vinod Raman, Seamus Somerstep, and Unique Subedi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are tasked with predicting the minimum, average, and maximum temperature for the next five days for 20 different weather stations. This report summarizes our data pipeline and the models we used to tackle this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources and Preparation\n",
    "To leverage different strenghts of publicly available datasets, we used NOAA and Wunderground as our data sources in the following manners:\n",
    "\n",
    "- NOAA: used to acquire historical data going back to 1960\n",
    "- Wunderground: used to acquired recent data, not restricted in timespan in theory but only used for the final year of analysis in the models presented\n",
    "\n",
    "Acquiring data from NOAA was straightforward after doing a conversion of the airport codes to the codes in the NOAA database, using the code below. By default, results are saved into `data/raw_noaa`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scraped data for: PANC\n",
      "INFO:root:Scraped data for: KBOI\n",
      "INFO:root:Scraped data for: KORD\n",
      "INFO:root:Scraped data for: KDEN\n",
      "INFO:root:Scraped data for: KDTW\n",
      "INFO:root:Scraped data for: PHNL\n",
      "INFO:root:Scraped data for: KIAH\n",
      "INFO:root:Scraped data for: KMIA\n",
      "INFO:root:Scraped data for: KMIC\n",
      "INFO:root:Scraped data for: KOKC\n",
      "INFO:root:Scraped data for: KBNA\n",
      "INFO:root:Scraped data for: KJFK\n",
      "INFO:root:Scraped data for: KPHX\n",
      "INFO:root:Scraped data for: KPWM\n",
      "INFO:root:Scraped data for: KPDX\n",
      "INFO:root:Scraped data for: KSLC\n",
      "INFO:root:Scraped data for: KSAN\n",
      "INFO:root:Scraped data for: KSFO\n",
      "INFO:root:Scraped data for: KSEA\n",
      "INFO:root:Scraped data for: KDCA\n"
     ]
    }
   ],
   "source": [
    "station_code_to_noaa = {\n",
    "    \"PANC\": \"USW00026451\",\n",
    "    \"KBOI\": \"USW00024131\",\n",
    "    \"KORD\": \"USW00094846\",\n",
    "    \"KDEN\": \"USW00003017\",\n",
    "    \"KDTW\": \"USW00094847\",\n",
    "    \"PHNL\": \"USW00022521\",\n",
    "    \"KIAH\": \"USW00012960\",\n",
    "    \"KMIA\": \"USW00012839\",\n",
    "    \"KMIC\": \"USW00094960\",\n",
    "    \"KOKC\": \"USW00013967\",\n",
    "    \"KBNA\": \"USW00013897\",\n",
    "    \"KJFK\": \"USW00094789\",\n",
    "    \"KPHX\": \"USW00023183\",\n",
    "    \"KPWM\": \"USW00014764\",\n",
    "    \"KPDX\": \"USW00024229\",\n",
    "    \"KSLC\": \"USW00024127\",\n",
    "    \"KSAN\": \"USW00023188\",\n",
    "    \"KSFO\": \"USW00023234\",\n",
    "    \"KSEA\": \"USW00024233\",\n",
    "    \"KDCA\": \"USW00013743\",\n",
    "}\n",
    "\n",
    "base_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/\"\n",
    "\n",
    "os.makedirs(utils.raw_noaa_cache, exist_ok=True)\n",
    "for station_code in station_code_to_noaa:\n",
    "    url = f\"{base_url}/{station_code_to_noaa[station_code]}.dly\"\n",
    "    urllib.request.urlretrieve(url, os.path.join(utils.raw_noaa_cache, f\"{station_code}.dly\"))\n",
    "    logging.info(f\"Scraped data for: {station_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply had to parse the DLY files acquired from the NOAA database, which could easily be done as follows. By default, results are saved into `data/processed_noaa`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processed data for: PANC\n",
      "INFO:root:Processed data for: KBOI\n",
      "INFO:root:Processed data for: KORD\n",
      "INFO:root:Processed data for: KDEN\n",
      "INFO:root:Processed data for: KDTW\n",
      "INFO:root:Processed data for: PHNL\n",
      "INFO:root:Processed data for: KIAH\n",
      "INFO:root:Processed data for: KMIA\n",
      "INFO:root:Processed data for: KMIC\n",
      "INFO:root:Processed data for: KOKC\n",
      "INFO:root:Processed data for: KBNA\n",
      "INFO:root:Processed data for: KJFK\n",
      "INFO:root:Processed data for: KPHX\n",
      "INFO:root:Processed data for: KPWM\n",
      "INFO:root:Processed data for: KPDX\n",
      "INFO:root:Processed data for: KSLC\n",
      "INFO:root:Processed data for: KSAN\n",
      "INFO:root:Processed data for: KSFO\n",
      "INFO:root:Processed data for: KSEA\n",
      "INFO:root:Processed data for: KDCA\n"
     ]
    }
   ],
   "source": [
    "from data.process_noaa import read_noaa_data_file\n",
    "\n",
    "def process_noaa(station):\n",
    "    noaa_path = os.path.join(utils.raw_noaa_cache, f\"{station}.dly\")\n",
    "    noaa_data = read_noaa_data_file(noaa_path)\n",
    "\n",
    "    os.makedirs(utils.processed_noaa_cache, exist_ok=True)\n",
    "    noaa_out_path = os.path.join(utils.processed_noaa_cache, f\"{station}.csv\")\n",
    "    noaa_data.to_csv(noaa_out_path)\n",
    "\n",
    "for station in utils.stations:\n",
    "    process_noaa(station)\n",
    "    logging.info(f\"Processed data for: {station}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquisition from Wunderground, however, was complicated by several issues. These, however, were worth navigating, since Wunderground was ultimately used as the ground truth data source in the final evaluation. The source of the complication was that Wunderground fetches its content dynamically, making a naive web scraper useless to get the data, as seen initially in loading a Wunderground page:\n",
    "\n",
    "![](dynamic.png)\n",
    "\n",
    "To circumvent this, we found the resources that were being fetched by the page by looking at the resources through Google Chrome's network analysis:\n",
    "\n",
    "![](api.png)\n",
    "\n",
    "From this, we were able to find that the contents of the hourly data, shown at the bottom of the Wunderground page, is fetched from a weather.com API endpoint. From the network analysis, we found the corresponding cURL command to fetch this content directly:\n",
    "\n",
    "![](command.png)\n",
    "\n",
    "With this cURL command in hand, we could then fetch these programmatically in Python by directly querying the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wunderground(station, end_date_str=\"2022-11-03\", download_window=5):\n",
    "    \"\"\"Downloads data from Wunderground from end_date-download_window to end_date. Note that\n",
    "    using too large a download_window (i.e. > 20) will cause this to error out\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"106\", \"Google Chrome\";v=\"106\", \"Not;A=Brand\";v=\"99\"',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Referer': 'https://www.wunderground.com/',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "    }\n",
    "\n",
    "    end_date = datetime.datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "    end_date = min(end_date, datetime.date.today()) # can't retrieve future days!\n",
    "\n",
    "    start_date = end_date - datetime.timedelta(days=(download_window-1))\n",
    "    start_date_str = f\"{start_date:%Y%m%d}\"\n",
    "    end_date_str = f\"{end_date:%Y%m%d}\"\n",
    "\n",
    "    params = {\n",
    "        'apiKey': 'e1f10a1e78da46f5b10a1e78da96f525',\n",
    "        'units': 'e',\n",
    "        'startDate': start_date_str,\n",
    "        'endDate': end_date_str,\n",
    "    }\n",
    "    response = requests.get(f'https://api.weather.com/v1/location/{station}:9:US/observations/historical.json', params=params, headers=headers)\n",
    "    return json.loads(response.text)[\"observations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some care had to be taken in making the requests, since sending one cURL request per day would be very inefficient: instead, we are able to batch requests and send them in parallel. Note that this requires multiple cores, meaning the final version run through Docker (if restricted to a single CPU) does not leverage this. However, this part of the data fetching processing is all done offline, meaning we can take advantage of multiple cores in this case. By default, results are saved into `data/raw_wunderground`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wunderground_pd_window(window_idx, start_date, station, download_window):\n",
    "    \"\"\"Downloads a *single window* from Wunderground. This is required since the API blocks large\n",
    "    simultaneous download requests, so large windows need to be broken into smaller requests. Note that\n",
    "    the recommended use is to invoke this indirectly through populate_wunderground_data_wrapper with\n",
    "    multiprocessing to parallelize HTTP requests by using fetch_wunderground_pd instead. See eval.py for an example\n",
    "\n",
    "    args:\n",
    "        window_idx: (int) which window is being downloaded (set to 0 if not parallelizing)\n",
    "        start_date: (datetime) \"current\" date: note that the past data can be scraped with window_idx < 0\n",
    "        station: (str) which station\n",
    "        download_window: (int) size of the window\n",
    "    \"\"\"\n",
    "    window_days = datetime.timedelta(days=download_window)\n",
    "    prediction_date = start_date + window_idx * window_days\n",
    "    end_date_str = f\"{prediction_date:%Y-%m-%d}\"\n",
    "    logging.info(f\"Requesting date: {end_date_str}\")\n",
    "    \n",
    "    wunderground_raw_data = fetch_wunderground(station=station, end_date_str=f\"{prediction_date:%Y-%m-%d}\", download_window=download_window)\n",
    "    wunderground_data = pd.DataFrame(wunderground_raw_data)\n",
    "    wunderground_data[\"date\"] = wunderground_data[\"valid_time_gmt\"].apply(lambda d: datetime.datetime.fromtimestamp(d))\n",
    "    wunderground_data = wunderground_data.set_index(\"date\")\n",
    "    # ARGHHH, the column is named \"GMT\" but it's actually the local time zone!!\n",
    "    wunderground_data.index = wunderground_data.index.tz_localize(\"EST\")\n",
    "    \n",
    "    return wunderground_data\n",
    "\n",
    "def fetch_wunderground_pd_window_wrapper(args):\n",
    "  return fetch_wunderground_pd_window(*args)\n",
    "\n",
    "def fetch_wunderground_pd(station, predict_date, future_days, past_days, ignore_cache=False):\n",
    "    \"\"\"Downloads Wunderground raw data and constructs a full dataset of the following\n",
    "    window: [predict_date - past_days, predict_date + future_days] \n",
    "\n",
    "    args:\n",
    "        station: (str) which station\n",
    "        predict_date: (datetime) \"current\" date of prediction\n",
    "        future_days: (int) how many days in the future to scrape (note: this can obviously only be\n",
    "            used for historical data, i.e. for evaluation tasks)\n",
    "        past_days: (int) how many days in the past to scrape\n",
    "    \"\"\"\n",
    "    os.makedirs(utils.raw_wunderground_cache, exist_ok=True)\n",
    "    # a bit messy, but to reuse the caching logic between eval and the \"final prediction,\" we have a special\n",
    "    # case for simplifying the name in the \"final prediction\" case to match the NOAA caching convention\n",
    "    if predict_date == datetime.date.today():\n",
    "        cache_fn = os.path.join(utils.raw_wunderground_cache, f\"{station}.csv\")\n",
    "    else:\n",
    "        cache_fn = os.path.join(utils.raw_wunderground_cache, f\"{station}-{predict_date:%Y-%m-%d}-{future_days}-{past_days}.csv\")\n",
    "\n",
    "    start = time.time()\n",
    "    if not ignore_cache and os.path.exists(cache_fn):\n",
    "        full_wunderground = pd.read_csv(cache_fn, index_col=0)\n",
    "        full_wunderground.index = pd.to_datetime(full_wunderground.index)\n",
    "    else:\n",
    "        download_window = 30\n",
    "        num_future_requests = ceil(future_days / download_window)\n",
    "        num_past_requests = -ceil(past_days / download_window)\n",
    "\n",
    "        p = Pool(multiprocessing.cpu_count())\n",
    "        populate_data_args = [(i, predict_date, station, download_window) for i in range(num_past_requests, num_future_requests + 1)]\n",
    "        full_wunderground = p.map(fetch_wunderground_pd_window_wrapper, populate_data_args)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        \n",
    "        full_wunderground = list(full_wunderground)\n",
    "        full_wunderground = pd.concat(full_wunderground)\n",
    "        if not ignore_cache:\n",
    "            full_wunderground.to_csv(cache_fn)\n",
    "    end = time.time()\n",
    "    logging.info(f\"Scraped data in: {end - start} s\")\n",
    "    return full_wunderground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.295526742935181 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.264394998550415 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.179196119308472 s\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 3.7278358936309814 s\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.163996934890747 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Scraped data in: 3.725299119949341 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.651342153549194 s\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.186283111572266 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.46047306060791 s\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.3150670528411865 s\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 121.91749906539917 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 3.555032968521118 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 3.5112109184265137 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.731019020080566 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 3.7736761569976807 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 7.334221124649048 s\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 3.97326397895813 s\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.1875059604644775 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.345566987991333 s\n",
      "INFO:root:Requesting date: 2021-11-11\n",
      "INFO:root:Requesting date: 2021-12-11\n",
      "INFO:root:Requesting date: 2022-01-10\n",
      "INFO:root:Requesting date: 2022-02-09\n",
      "INFO:root:Requesting date: 2022-03-11\n",
      "INFO:root:Requesting date: 2022-04-10\n",
      "INFO:root:Requesting date: 2022-05-10\n",
      "INFO:root:Requesting date: 2022-06-09\n",
      "INFO:root:Requesting date: 2022-07-09\n",
      "INFO:root:Requesting date: 2022-08-08\n",
      "INFO:root:Requesting date: 2022-09-07\n",
      "INFO:root:Requesting date: 2022-10-07\n",
      "INFO:root:Requesting date: 2022-11-06\n",
      "INFO:root:Requesting date: 2022-12-06\n",
      "INFO:root:Scraped data in: 4.460571765899658 s\n"
     ]
    }
   ],
   "source": [
    "for station in utils.stations:\n",
    "    wunderground_days = 365 # get one year of past data\n",
    "    fetch_wunderground_pd(station, predict_date=datetime.date.today(), future_days=0, past_days=wunderground_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this fetches a list of the *hourly* measurements, i.e. temperature, wind speed, wind direction, etc... However, the final task was specificallly interested in measuring attributes at the daily time scale, meaning such granular measurements would not be useful. Unlike the hourly contents itself, the Wunderground site does *not* retrieve its daily summaries from an endpoint: after all, doing so would be redundant given that the latter can be directly computed from the former. Therefore, the Wunderground site both dynamically fetches the hourly content and then computes the daily summaries that are shown. We, therefore, had to replicate this ourselves to get the min, max, and average temperatures for training. \n",
    "\n",
    "This naively could be done simply by aggregating the contents to each unique day in the dataset and taking the min, max, and average across such datapoints. However, naively doing so produces incorrect answers, since the data are *returned in a single time zone* while the summative values are *computed based on the time zone of the station*, i.e. Alaska's min, max, and average will be taken in the Alaska/Anchroage time zone. Similar to the hourly data, the timezone could be queried using a separate endpoint, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_latlon_cache = {}\n",
    "station_timezone_cache = {}\n",
    "\n",
    "def fetch_latlon(station):\n",
    "    if station not in station_latlon_cache:\n",
    "        dummy_url = f\"https://www.wunderground.com/history/daily/{station}/date/2021-11-05\"\n",
    "        ans = requests.get(dummy_url)\n",
    "        script_vals = ans.text.split(\"&q;:\")\n",
    "        extract_lat_or_lon = lambda lat_or_lon : float([script_vals[i+1] for i, v in enumerate(script_vals) if lat_or_lon in v][0].split(\",\")[0])\n",
    "        station_latlon_cache[station] = (extract_lat_or_lon(\"latitude\"), extract_lat_or_lon(\"longitude\"))\n",
    "    return station_latlon_cache[station]\n",
    "\n",
    "def fetch_timezone(station):\n",
    "    if station not in station_latlon_cache:\n",
    "        lat_lon = fetch_latlon(station)\n",
    "\n",
    "        headers = {\n",
    "            'sec-ch-ua': '\"Google Chrome\";v=\"107\", \"Chromium\";v=\"107\", \"Not=A?Brand\";v=\"24\"',\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Referer': 'https://www.wunderground.com/',\n",
    "            'sec-ch-ua-mobile': '?0',\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "            'sec-ch-ua-platform': '\"macOS\"',\n",
    "        }\n",
    "\n",
    "        params = {\n",
    "            'apiKey': 'e1f10a1e78da46f5b10a1e78da96f525',\n",
    "            'geocode': f'{round(lat_lon[0], 2)},{round(lat_lon[1], 2)}',\n",
    "            'format': 'json',\n",
    "        }\n",
    "\n",
    "        response = requests.get('https://api.weather.com/v3/dateTime', params=params, headers=headers)\n",
    "        station_timezone_cache[station] = json.loads(response.text)[\"ianaTimeZone\"]\n",
    "    return station_timezone_cache[station]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can finally do the aggregation as follows. By default, results are saved into `data/processed_wunderground`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wunderground_df(raw_wunderground_data, station):\n",
    "    local_timezone = pytz.timezone(utils.fetch_timezone(station))\n",
    "    raw_wunderground_data['date_col'] = pd.to_datetime(raw_wunderground_data.index).tz_convert(local_timezone).date\n",
    "        \n",
    "    aggregated_columns = [\"temp\", \"wspd\", \"pressure\", \"heat_index\", 'dewPt']\n",
    "    maxes = raw_wunderground_data.groupby(['date_col'], sort=False)[aggregated_columns].max().set_axis([f\"{column}_max\" for column in aggregated_columns], axis=1, inplace=False).set_index(raw_wunderground_data['date_col'].unique())\n",
    "    means = raw_wunderground_data.groupby(['date_col'], sort=False)[aggregated_columns].mean().set_axis([f\"{column}_mean\" for column in aggregated_columns], axis=1, inplace=False).set_index(raw_wunderground_data['date_col'].unique())\n",
    "    mins  = raw_wunderground_data.groupby(['date_col'], sort=False)[aggregated_columns].min().set_axis([f\"{column}_min\" for column in aggregated_columns], axis=1, inplace=False).set_index(raw_wunderground_data['date_col'].unique())\n",
    "    wind_dir = raw_wunderground_data.groupby(['date_col'], sort=False)['wdir_cardinal'].agg(\n",
    "        lambda x: pd.Series.mode(x)[0]).astype(\"category\").to_frame(\"wdir_mode\").set_index(raw_wunderground_data['date_col'].unique())\n",
    "    return pd.concat((mins, means, maxes, wind_dir), axis=1)\n",
    "\n",
    "def process_wunderground(station):\n",
    "    \"\"\"Wunderground returns granular (hourly) data points, but we only want daily \n",
    "    for prediction: this coarsens the dataset\n",
    "    \"\"\"\n",
    "    wunderground_path = os.path.join(utils.raw_wunderground_cache, f\"{station}.csv\")\n",
    "    raw_wunderground_data = pd.read_csv(wunderground_path, index_col=0)\n",
    "    raw_wunderground_data.index = pd.to_datetime(raw_wunderground_data.index)\n",
    "    \n",
    "    processed_wunderground = process_wunderground_df(raw_wunderground_data, station)\n",
    "    os.makedirs(utils.processed_wunderground_cache, exist_ok=True)\n",
    "    wunderground_out_path = os.path.join(utils.processed_wunderground_cache, f\"{station}.csv\")\n",
    "    processed_wunderground.to_csv(wunderground_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processed data for: PANC\n",
      "INFO:root:Processed data for: KBOI\n",
      "INFO:root:Processed data for: KORD\n",
      "INFO:root:Processed data for: KDEN\n",
      "INFO:root:Processed data for: KDTW\n",
      "INFO:root:Processed data for: PHNL\n",
      "INFO:root:Processed data for: KIAH\n",
      "INFO:root:Processed data for: KMIA\n",
      "INFO:root:Processed data for: KMIC\n",
      "INFO:root:Processed data for: KOKC\n",
      "INFO:root:Processed data for: KBNA\n",
      "INFO:root:Processed data for: KJFK\n",
      "INFO:root:Processed data for: KPHX\n",
      "INFO:root:Processed data for: KPWM\n",
      "INFO:root:Processed data for: KPDX\n",
      "INFO:root:Processed data for: KSLC\n",
      "INFO:root:Processed data for: KSAN\n",
      "INFO:root:Processed data for: KSFO\n",
      "INFO:root:Processed data for: KSEA\n",
      "INFO:root:Processed data for: KDCA\n"
     ]
    }
   ],
   "source": [
    "for station in utils.stations:\n",
    "    process_wunderground(station)\n",
    "    logging.info(f\"Processed data for: {station}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the processed data, we wished to produce datasets for cross validation that parallel the final test set, which was to be Nov 30-Dec 10. The most natural evaluation test, therefore, would be looking at the same timeframe across multiple years. We start by fetching *all* the data necessary to construct this evaluation task. In particular, for a window of Nov 30-Dec 10 2021, we would need Wunderground data from Nov 30 **2020** - Dec 15 **2021**. Note that how far we extend back is arbitrary (training could have extended for more than one year) and that we also need data 5 days beyond the end of the evaluation period, since for *each day* in the evaluation window, we predict the following 5 days min, max, and average temperatures. Therefore, we start by fetching this overall dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_full_eval_data(start_eval_date, eval_len, wunderground_lookback):\n",
    "    \"\"\"Prepares data for evaluation for a window of [start_eval_date, start_eval_date + eval_len] \n",
    "    where eval_len is to be specified as the number of days. Note that the data returned from this\n",
    "    is NOT the data that is to be used for evaluation, i.e. each eval_day must be separated after\n",
    "    this initial bulk fetch (using get_eval_task)\n",
    "    \n",
    "    args:\n",
    "        start_eval_date: (datetime.datetime) day of first *evaluation*, i.e. first day where predictions are *made*\n",
    "            Note: that EACH eval day is evaluated for 5 days forward!\n",
    "        eval_len: how many eval days to include\n",
    "        wunderground_lookback: how far (in days) *before the first eval day* to extend the Wunderground data\n",
    "            Note: data scraping will take time proportional to this number\n",
    "    \"\"\"\n",
    "    noaa = utils.load_processed_data_src(\"noaa\")\n",
    "    full_eval_data = {}\n",
    "    for station in utils.stations:\n",
    "        full_eval_data[station] = {}\n",
    "        full_eval_data[station][\"noaa\"] = noaa[station]\n",
    "        full_eval_data[station][\"wunderground\"] = wunderground_download.fetch_wunderground_pd(\n",
    "            station, start_eval_date, eval_len, wunderground_lookback)\n",
    "    return full_eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, for each day in the evaluation window, we artifically chop off the dataset to simulate as if we were predicting from that day of interest. For example, if we are doing predictions on Dec 1 2021, we return the dataset view from Nov 30 2020 - noon EST Dec 1 2021. NOAA is returned up to 3 days from the prediction day, since we NOAA is known to lag by about 3 days from the current day. For each day, the \"target\" was then compiled to be the `[min, mean, max]` temperatures for the five days looking forward from the prediction day for each of the stations, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_eval_task(full_eval_data, prediction_date, station):\n",
    "    full_noaa = full_eval_data[station][\"noaa\"]\n",
    "    full_wunderground = full_eval_data[station][\"wunderground\"]\n",
    "\n",
    "    est = pytz.timezone('US/Eastern')\n",
    "    strict_cutoff = est.localize(prediction_date.replace(hour=12)) # all the predictions are going to be made noon EST\n",
    "\n",
    "    local_timezone = pytz.timezone(utils.fetch_timezone(station))\n",
    "    full_wunderground['date_col'] = pd.to_datetime(full_wunderground.index).tz_convert(local_timezone).date\n",
    "    \n",
    "    # cutoff_side = 0: < \"prediction cutoff\" -- used to construct our dataset\n",
    "    # cutoff_side = 1: > \"prediction cutoff\" -- used to construct the evaluation target\n",
    "    for cutoff_side in range(2):\n",
    "        if cutoff_side == 0:\n",
    "            dataset_view = full_wunderground[full_wunderground.index < strict_cutoff]\n",
    "        else:\n",
    "            dataset_view = full_wunderground[full_wunderground.index >= strict_cutoff]\n",
    "\n",
    "        # Wunderground returns granular (hourly) data points, but we only want daily for prediction: this coarsens the dataset\n",
    "        # TODO: time permitting, could remove this since it is a bit of a duplicate from process_wunderground\n",
    "        aggregated_columns = [\"temp\", \"wspd\", \"pressure\", \"heat_index\", 'dewPt']\n",
    "        maxes = dataset_view.groupby(['date_col'], sort=False)[aggregated_columns].max().set_axis([f\"{column}_max\" for column in aggregated_columns], axis=1, inplace=False).set_index(dataset_view['date_col'].unique())\n",
    "        means = dataset_view.groupby(['date_col'], sort=False)[aggregated_columns].mean().set_axis([f\"{column}_mean\" for column in aggregated_columns], axis=1, inplace=False).set_index(dataset_view['date_col'].unique())\n",
    "        mins  = dataset_view.groupby(['date_col'], sort=False)[aggregated_columns].min().set_axis([f\"{column}_min\" for column in aggregated_columns], axis=1, inplace=False).set_index(dataset_view['date_col'].unique())\n",
    "        wind_dir = dataset_view.groupby(['date_col'], sort=False)['wdir_cardinal'].agg(\n",
    "            lambda x: pd.Series.mode(x)[0]).astype(\"category\").to_frame(\"wdir_mode\").set_index(dataset_view['date_col'].unique())\n",
    "        aggregated_wunderground = pd.concat((mins, means, maxes, wind_dir), axis=1)\n",
    "\n",
    "        if cutoff_side == 0:\n",
    "            cut_wunderground = aggregated_wunderground.drop(aggregated_wunderground.index[0], axis=0) # first row is often partial day based on the time zone\n",
    "        else:\n",
    "            evaluation_data = aggregated_wunderground\n",
    "\n",
    "    noaa_cutoff_len = 3\n",
    "    noaa_cutoff = prediction_date - datetime.timedelta(days=noaa_cutoff_len)\n",
    "    cut_noaa = full_noaa.iloc[full_noaa.index < noaa_cutoff]\n",
    "    \n",
    "    forecast_horizon = 5\n",
    "    prediction_window = [prediction_date + datetime.timedelta(days=forecast_day) for forecast_day in range(1, forecast_horizon + 1)]\n",
    "    prediction_targets_df = evaluation_data.loc[prediction_window]\n",
    "    target = []\n",
    "    for i in range(len(prediction_targets_df)):\n",
    "        target.append(prediction_targets_df[\"temp_min\"][i])\n",
    "        target.append(prediction_targets_df[\"temp_mean\"][i])\n",
    "        target.append(prediction_targets_df[\"temp_max\"][i])\n",
    "    target = np.array(target)\n",
    "\n",
    "    data = {\n",
    "        \"noaa\": cut_noaa,\n",
    "        \"wunderground\": cut_wunderground,\n",
    "    }\n",
    "    return data, target\n",
    "\n",
    "def get_eval_task(full_eval_data, prediction_date):\n",
    "    full_data = {}\n",
    "    full_target = []\n",
    "    for station in utils.stations:\n",
    "        data, target = get_station_eval_task(full_eval_data, prediction_date, station)\n",
    "        full_data[station] = data\n",
    "        full_target.append(target.flatten())\n",
    "    full_target = np.array(full_target).flatten()\n",
    "    return full_data, full_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we had a unified framework for evaluating models: given the evaluation period, we take the chopped dataset, feed that to the model being evaluated, and compute the MSE as our evaluation metric of the predicted against the target min, mean, and max daily temperatures, which was repeated for multiple years to assess the robustness of findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_single_window(start_eval_date, eval_len, wunderground_lookback, model):\n",
    "    \"\"\"Runs an evaluation for a window of [start_eval_date, start_eval_date + eval_len] \n",
    "    where eval_len is to be specified as the number of days\n",
    "    \n",
    "    args:\n",
    "        start_eval_date: (datetime.datetime) day of first *evaluation*, i.e. first day where predictions are *made*\n",
    "            Note: that EACH eval day is evaluated for 5 days forward!\n",
    "        eval_len: how many eval days to include\n",
    "        wunderground_lookback: how far (in days) *before the first eval day* to extend the Wunderground data\n",
    "            Note: data scraping will take time proportional to this number\n",
    "    \"\"\"\n",
    "    full_eval_data = prepare_full_eval_data(start_eval_date, eval_len, wunderground_lookback)\n",
    "    \n",
    "    mses = []\n",
    "    for day_offset in range(eval_len):\n",
    "        prediction_date = start_eval_date + datetime.timedelta(days=day_offset)\n",
    "        eval_data, eval_target = get_eval_task(full_eval_data, prediction_date)\n",
    "        \n",
    "        predictions = model.predict(eval_data)\n",
    "        mse = (np.square(eval_target - predictions)).mean()\n",
    "        print(mse)\n",
    "        mses.append(mse)\n",
    "    return mses\n",
    "\n",
    "def eval(model):\n",
    "    \"\"\"Runs evaluations for a windows from 11/30 - 12/10 for multiple years (default: 10 years) \n",
    "    using the specified model as the predictor. Returns MSEs as a 20 x 15 matrix, with each station a row\n",
    "    across the 10 years with the year as the key of a dict, i.e.:\n",
    "    \n",
    "    {\n",
    "        2012: [MSEs],\n",
    "        2013: [MSEs],\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    start_year = 2017\n",
    "    num_years = 5\n",
    "    mses_per_year = {}\n",
    "    wunderground_lookback = 365 # how many days back to return of wunderground data\n",
    "    eval_len = 10 # how many days we running evaluation for\n",
    "    \n",
    "    for year in range(start_year, start_year + num_years):\n",
    "        start_eval_str = f\"{year}-11-30\" # when eval period starts (must follow %Y-%m-%d format)\n",
    "        start_eval_date = datetime.datetime.strptime(start_eval_str, \"%Y-%m-%d\") \n",
    "        mses_per_year[year] = eval_single_window(start_eval_date, eval_len, wunderground_lookback, model)\n",
    "    return mses_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we discuss simple, computationally-efficient baseline models for predicting the minimum, average, and maximum temperatures for the next five days. These simple models enable us to better evaluate the performance of the more sophisticated models we tried. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Day Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first baseline model, termed the Previous Day Predictor, exploits the idea that for every station, today's minimum, average, and maximum tempertures are good estimates for the minimum, average, and maximum temperates for *each* of the next five days. To this end, the Previous Day Predictor predicts for each station, for each of the next five days, today's minimum, average, and maximum temperture as the respective minimum, average, and maximum temperature for that day. That is, for each station, the Previous Day Predictor outputs a vector of 15 values consisting of today's minimum, average, and maximum temperatures repeated five times in a row. The model class below formalizes this idea in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevDayPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, data):\n",
    "        stations_data = [list(data[station][\"wunderground\"].iloc[-1][[\"temp_min\",\"temp_mean\",\"temp_max\"]].values) * 5 for station in utils.stations]\n",
    "        stations_data = np.array(stations_data).flatten()\n",
    "        return stations_data\n",
    "\n",
    "prev_day = PrevDayPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Wunderground data is used to compute \"today's\" minimum, average, and maximum temperature since the NOAA dataset lags by a couple days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical Average Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictor.models.predictor_scaffold import Predictor\n",
    "class HistoricAveragePredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        stations_data = []\n",
    "        for station in utils.stations:\n",
    "            noaa = data[station][\"noaa\"]\n",
    "            noaa = noaa.loc[:, [ \"TMIN\", \"TAVG\", \"TMAX\"]].dropna(axis =0)\n",
    "            noaa = noaa*0.18+32.0\n",
    "            current_date = noaa.index[-1]\n",
    "       \n",
    "            df = noaa.groupby(by=[noaa.index.month, noaa.index.day]).mean().round(2)\n",
    "            \n",
    "            for i in range(1, 6):\n",
    "                date_ = (current_date + pd.DateOffset(days=i))\n",
    "                stations_data.append(df[df.index == (date_.month, date_.day)].values.flatten())\n",
    "           \n",
    "       \n",
    "             \n",
    "           \n",
    "        stations_data = np.array(stations_data).flatten()\n",
    "        return stations_data\n",
    "\n",
    "historic = HistoricAveragePredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to make a prediction for Dec 1. The rationale for this model is that the temperature of this year's Dec 1 will be similar to that of past Dec 1's. In particular, let $y_{i}$ be the maximum temperature of Dec 1 for year $i$. In NOAA dataset, we have $1941 \\leq i \\leq 2021$. Then, the prediction of maximum temperature of Dec 1, 2022 is \n",
    "$$\\frac{1}{2021-1941+1} \\sum_{i=1941}^{2021}y_i.$$\n",
    "So, our baseline is to predict historic average of minimum, average, and maximum temperatures for each station on that particular day. Unfortunately, this baseline did not end up performing that well for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weighted Average baseline predictor interpolates between the Previous Day Predictor and the Historical Average Predictor. At its core, it exploits the idea that tomorrow's temperature will be similar to today's temperature but the temperate 5 days from now will be more similar to the historical average temperature for that day. To this end, the Weighted Average Predictor computes a weighted average between today's temperature and the historical average temperature for each day and station. In other words, for each station, for each day, for each measurement (i.e., min, avg, max), the Weighted Average Predictor computes a weighted average between today's temperature and the historical average for the corresponding day. The model class below formalizes this idea in code. Note that it takes in as input a fixed set of mixing weights and uses the PreviousDayPredictor and the Historical Average Predictor as black-boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevDayHistoricalPredictor(Predictor):\n",
    "    def __init__(self, weights):\n",
    "        self.prev_day_model = PrevDayPredictor()\n",
    "        self.historical_day_model = HistoricAveragePredictor()\n",
    "        self.weights = weights\n",
    "\n",
    "    def predict(self, data):\n",
    "        weights = self.weights\n",
    "\n",
    "        prev_day_pred = self.prev_day_model.predict(data)\n",
    "        historical_day_pred = self.historical_day_model.predict(data)\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        for index in range(len(prev_day_pred)//3):\n",
    "            prev_day_val = prev_day_pred[index*3:index*3 + 3]\n",
    "            historical_day_val= historical_day_pred[index*3:index*3 + 3]\n",
    "\n",
    "            prev_day_val_weighted = prev_day_val * weights[counter]\n",
    "            historical_day_val_weighted = historical_day_val * (1-weights[counter])\n",
    "\n",
    "            prev_day_pred[index*3:index*3 + 3] = prev_day_val_weighted\n",
    "            historical_day_pred[index*3:index*3 + 3] = historical_day_val_weighted\n",
    "\n",
    "            counter += 1\n",
    "            counter %= 5\n",
    "\n",
    "        predictions = (prev_day_pred + historical_day_pred).round(1)\n",
    "        return predictions\n",
    "\n",
    "WA_pred = PrevDayHistoricalPredictor(weights=[1, 0.80, 0.60, 0.40, 0.20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Weighted Average Predictor uses the same fixed set of 5 mixing weights between today's temperature and the historical average temperature across all stations and measurements.  That is, there is a single mixing weight associated to each day out.  The table below provides the mixing weights used in the instantiation WA_pred. \n",
    "\n",
    "| Days out | Weight on Today's temperature  | Weight on Historical Average temperature |\n",
    "|----------|--------------------------------|------------------------------------------|\n",
    "| 1        | 1                              | 0                                        |\n",
    "| 2        | 0.80                           | 0.20                                     |\n",
    "| 3        | 0.60                           | 0.40                                     |\n",
    "| 4        | 0.40                           | 0.60                                     |\n",
    "| 5        | 0.20                           | 0.80                                     |\n",
    "\n",
    "Observe that the weiht on Today's temperature decreases as the number of days out increases and vice versa for the weight on the historical average temperature. This coincides with the idea that tomorrow's temperature is most similar to today's temperature, but the temperature five days from now will be more similar to the historical average temperature for that day. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA, which stands for Auto-regressive Integrated Moving Average, are general models for time series forecasting. The ARIMA model for stationary time series are linear regression models and one set of its predictors are lags of the variables to be predicted.  A key concept in time series modeling is stationarity, which roughly says that the statistical properties of the time series is constant over time. The time series may not be stationary as it is, so we have to apply some linear/non-linear transformation to the data to bring stationarity. For example, differencing is a popular technique to bring stationarity. Suppose $Y_t$ be the maximum temperature at time stamp $t$. Then, the time series $\\{Y_t\\}_{t \\geq 1}$ may not be stationary, but the differenced variable $y_t = Y_{t} - Y_{t-1}$ perhaps is. Generally, we may have to use higher order of differencing. For example a second order differencing involves transforming time series to $y_{t} = (Y_{t} - Y_{t-1}) - (Y_{t-1} - Y_{t-2})$. On top of lagged difference of the variable, this model can also use the forecast errors in the past time steps, which are called moving average components. These errors denoted by $\\{\\epsilon_t\\}_{t \\geq 1}$ are assumped to to iid $\\text{Normal}(0,1)$. Therefore, an ARIMA model with $p$ auto-regressive component, $d$ order of differencing, and $q$ moving average components is \n",
    "$$y_{t} = \\mu + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} +\\ldots + \\beta_{p} y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\ldots + \\theta_{q} \\epsilon_{t-q},$$\n",
    "where $y_{t} = (Y_{t}- Y_{t-1}) - \\ldots - (Y_{t-d+1} - Y_{t-d}).$ We use ARIMA($p,d, q$) to denote this model.\n",
    "\n",
    "We trained three ARIMA models per station each for minimum, average, and maximum temperature. Thus, we have 60 ARIMA models all together for 20 stations. Each model forecast respective temperature for next five days. The model class below formalizes this idea in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "class ArimaPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        stations_data = []\n",
    "        for station in utils.stations:\n",
    "\n",
    "            df = data[station][\"wunderground\"]\n",
    "            temp_max = df['temp_max'].asfreq('D')\n",
    "            temp_min = df['temp_min'].asfreq('D')\n",
    "            temp_avg = df['temp_mean'].asfreq('D')\n",
    "            \n",
    "\n",
    "            min_model = ARIMA(temp_min, order=(3,1,1))\n",
    "           \n",
    "            avg_model = ARIMA(temp_avg, order=(3,1,1))\n",
    "            max_model = ARIMA(temp_max, order=(3,1,1))\n",
    "            \n",
    "            min_fc = min_model.forecast(steps = 5)\n",
    "            avg_fc = avg_model.forecast(steps = 5)\n",
    "            max_fc  = max_model.forecast(steps = 5)\n",
    "            \n",
    "    \n",
    "            stations_data.append(np.vstack((min_fc.values, avg_fc.values, max_fc.values)).flatten())\n",
    "             \n",
    "           \n",
    "        stations_data = np.array(stations_data).flatten()\n",
    "        return stations_data\n",
    "\n",
    "arima = ArimaPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We obtained decent results with ARIMA model, but not in par with our regression based models. Moreover, we tried ARIMA models that takes into account various seasonal trends (monthly, yearly, weekly), but the model fitting took much longer with only marginal improvements in the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we present prediction models based on Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for Regression-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our regression-based models, we exclusively used data from Wunderground.  Data from this source is returned hourly so for feature preperation we aggregated the into daily data using the following calculations.  Average temperature is calculated as the average of the hourly temperatures while max/min temperature are calculated as the highest and lowest hourly temperature over the day respectively. Heat index, dew point, pressure and wind speed are aggregated in a similar manner.  Note that heat index is a human percieved temperature feature which combines heat with humidity.  The dew point is the temperature at which the relative humidity becomes 100%.  Finally wind direction was computed as the mode hourly wind direction from the possible categories of (CALM, S, SSE,SEE, E, NEE, NNE, N, NNW, NWW, W, SWW, SSW, VAR). Below is an aggregated table of the features used.\n",
    "\n",
    "| Features                     |\n",
    "|------------------------------|\n",
    "| Daily Min/Max/Avg Temp       |\n",
    "| Daily Min/Max/Avg Pressure   |\n",
    "| Daily Min/Max/Avg Heat Index |\n",
    "| Daily Min/Max/Avg Dew Point  |\n",
    "| Daily Min/Max/Avg Wind Speed |\n",
    "| Daily Mode Wind Direction    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below gives a snapshot of the Wunderground dataframe for a particular station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_min</th>\n",
       "      <th>wspd_min</th>\n",
       "      <th>pressure_min</th>\n",
       "      <th>heat_index_min</th>\n",
       "      <th>dewPt_min</th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>wspd_mean</th>\n",
       "      <th>pressure_mean</th>\n",
       "      <th>heat_index_mean</th>\n",
       "      <th>dewPt_mean</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>wspd_max</th>\n",
       "      <th>pressure_max</th>\n",
       "      <th>heat_index_max</th>\n",
       "      <th>dewPt_max</th>\n",
       "      <th>wdir_mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-05</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.76</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>39.652174</td>\n",
       "      <td>5.043478</td>\n",
       "      <td>29.810435</td>\n",
       "      <td>39.652174</td>\n",
       "      <td>30.869565</td>\n",
       "      <td>44</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.88</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-06</th>\n",
       "      <td>42</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.35</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>17.625000</td>\n",
       "      <td>29.519167</td>\n",
       "      <td>43.875000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.76</td>\n",
       "      <td>46</td>\n",
       "      <td>36</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-07</th>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.36</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>44.107143</td>\n",
       "      <td>8.607143</td>\n",
       "      <td>29.472857</td>\n",
       "      <td>44.107143</td>\n",
       "      <td>36.964286</td>\n",
       "      <td>49</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.55</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>SSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-08</th>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.95</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>42.066667</td>\n",
       "      <td>6.733333</td>\n",
       "      <td>29.062333</td>\n",
       "      <td>42.066667</td>\n",
       "      <td>36.433333</td>\n",
       "      <td>44</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.31</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>SSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-09</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.96</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>43.511628</td>\n",
       "      <td>6.534884</td>\n",
       "      <td>29.249302</td>\n",
       "      <td>43.511628</td>\n",
       "      <td>38.697674</td>\n",
       "      <td>47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.51</td>\n",
       "      <td>47</td>\n",
       "      <td>41</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            temp_min  wspd_min  pressure_min  heat_index_min  dewPt_min  \\\n",
       "2021-10-05        35       0.0         29.76              35         27   \n",
       "2021-10-06        42       7.0         29.35              42         32   \n",
       "2021-10-07        41       0.0         29.36              41         34   \n",
       "2021-10-08        41       0.0         28.95              41         35   \n",
       "2021-10-09        42       0.0         28.96              42         36   \n",
       "\n",
       "            temp_mean  wspd_mean  pressure_mean  heat_index_mean  dewPt_mean  \\\n",
       "2021-10-05  39.652174   5.043478      29.810435        39.652174   30.869565   \n",
       "2021-10-06  43.875000  17.625000      29.519167        43.875000   34.000000   \n",
       "2021-10-07  44.107143   8.607143      29.472857        44.107143   36.964286   \n",
       "2021-10-08  42.066667   6.733333      29.062333        42.066667   36.433333   \n",
       "2021-10-09  43.511628   6.534884      29.249302        43.511628   38.697674   \n",
       "\n",
       "            temp_max  wspd_max  pressure_max  heat_index_max  dewPt_max  \\\n",
       "2021-10-05        44      12.0         29.88              44         35   \n",
       "2021-10-06        46      25.0         29.76              46         36   \n",
       "2021-10-07        49      21.0         29.55              49         39   \n",
       "2021-10-08        44      17.0         29.31              44         38   \n",
       "2021-10-09        47      10.0         29.51              47         41   \n",
       "\n",
       "           wdir_mode  \n",
       "2021-10-05        NE  \n",
       "2021-10-06        SE  \n",
       "2021-10-07       SSE  \n",
       "2021-10-08       SSE  \n",
       "2021-10-09         S  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = utils.load_processed_data()\n",
    "data[\"PANC\"][\"wunderground\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Regression Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the features above, we created a Regression data set $(X, y)$. The $ith$ row of $(X, y)$ is associated with a particular date $i$ such that $X[i]$ is a 48-dimensional vector containing the aforementioned features for the previous  $3$ days ($3 * 16 = 48$ covariates in total) and $y[i]$ is a 15-dimensional vector containing the daily minimum, average, and maximum temperatures for the next $5$ days. As an example, if today's date was 10/11/2020, then the corresponding rows in $X$ and $y$ will contain the features for the days 10/8/2020 -10/10/2020 and the temperatures for the days 10/12/2020 - 10/16/2020. The function below uses Wunderground data for a given station to construct and output a regression dataset for that particular station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_data(data, window_size, features):\n",
    "    if features is not None:\n",
    "        keep_data = data[features]\n",
    "    else:\n",
    "        keep_data = data\n",
    "        \n",
    "    X, y = [], []\n",
    "    target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "    prediction_window = 5\n",
    "    for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "        X.append(keep_data.values[i:i+window_size,:-1].flatten())\n",
    "        y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "    test_X = keep_data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "    return np.array(X), np.array(y), test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ```data``` contains the Wunderground data for a specific station. The input ```window_size``` controls how many days of features should be included in each row of $X$, but this was fixed as $3$ (as mentioend above). The input ```features``` controls which features (i.e. columns of $X$) should be kept when constructing the regression dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_data(data, window_size, features):\n",
    "    if features is not None:\n",
    "        keep_data = data[features]\n",
    "    else:\n",
    "        keep_data = data\n",
    "        \n",
    "    X, y = [], []\n",
    "    target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "    prediction_window = 5\n",
    "    for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "        X.append(keep_data.values[i:i+window_size,:-1].flatten())\n",
    "        y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "    test_X = keep_data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "    return np.array(X), np.array(y), test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Meta-Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to expediate the process of model exploration, we additionally implemented a MetaPredictor class which takes in as input a blackbox implementation of a regression model, and uses it to make predictions. That is, instead of having to create a a new model class for every type of Regression model we want to try, we can now simply instantiate and pass any regression model as input into the constructor of the MetaPredictor. The MetaPredictor will then train 20 copies of the specified regression model on the regression dataset created by calling ```create_regression_data``` for each station, and then use each of the trained models to make a prediction of the temperatures for the next $5$ days.  The model class below formalizes this idea in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaPredictor(Predictor):\n",
    "    def __init__(self, reg, window_size, features= None):\n",
    "        self.reg = reg\n",
    "        self.window_size = window_size\n",
    "        self.features = features\n",
    "\n",
    "    def predict(self, data):\n",
    "        stations_data = []\n",
    "        start = time.time()\n",
    "        for station in utils.stations:\n",
    "            window_size = self.window_size\n",
    "            X, y, test_X = create_regression_data(data[station][\"wunderground\"], window_size, self.features) \n",
    "            self.reg.fit(X, y)\n",
    "            stations_data.append(self.reg.predict(test_X))\n",
    "        end = time.time()\n",
    "        logging.info(f\"Performed prediction in: {end - start} s\")\n",
    "        return np.array(stations_data).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each time the function predict is called, the MetaPredictor, makes 20 regression datasets, one for each station, trains the specified regression model on each of the datasets, and uses the trained models to make predictions for the next 5 days for each station. This process is done sequentially, one station at a time. Note that each time ```self.reg.fit``` is called, it erases its previous memory, so that a fresh new model is trained for each station. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few sub-sections, we show how the MetaPredictor model class can be used to create a variety of different regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OLS, Lasso, and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Meta-Predictor, it is easy to construct predictors that use OLS, Lasso, and Ridge Regression models for predicting the temperatures for each station. In particular, the code below shows how to construct each of these regression models using the Meta-Predictor class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "window_size = 3\n",
    "ols = MetaPredictor(reg, window_size, keep_features)\n",
    "\n",
    "reg = RidgeCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1])\n",
    "window_size = 3\n",
    "ridge = MetaPredictor(reg, window_size, keep_features)\n",
    "\n",
    "reg = LassoCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1, 1])\n",
    "window_size = 3\n",
    "lasso = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by construction, the Meta-Predictor will train one regression model per station. Each regression model will take in as input the covariates previously specified, and output a 15-dimension vector corresponding to its predictions of the minimum, maximum, and average temperatures for the next 5 days. That is, for each station, the Meta-Predictor class will train a multi-output regression model. By using RidgeCV and LassoCV and passing in a list of candidate values for $\\alpha$, these sklearn model classes (RidgeCV, LassoCV) will automatically hyperparameter tune the regression model for each station. Once again, by construction, the Meta-Predictor will create a new dataset and train a new regression model for each station every time the predict function is called. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multilayer perceptron is a fully connected neural network. In particular, we used a two layer perceptron\n",
    "$$y  = W_2\\, \\sigma(\\, \\sigma(W_1x + b_1) +b_2 ), $$\n",
    "where the activation is relu. Each hidden layer is of dimension $20$ and the output layer has $15$ dimensions for min, average, and max of next five days. Since we trained one MLP model for one station, we trained 20 MLP regressors alltogether. The code below formalizes this idea by using the MLPRegressor class from sklearn and the aforementioned MetaPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor(random_state=1, hidden_layer_sizes=(20,20, ), max_iter=2000, solver='lbfgs', learning_rate= 'adaptive')\n",
    "window_size = 3\n",
    "mlp = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One main attraction of MLPs are that it allows for multitask regressions based on shared representations. It is particularly relevant here because the assumption of shared representation seems reasonable if we are predictive the response of same qualitative nature, temperature. We obtained pretty good results with this model, however, similar performance was obtained with ensemble methods with much less training time. So, we ended up choosing ensemble based models for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods, like Boosting and Bagging, are powerful prediction models for tabular data that are known to generalize well in practice. These types of models contain sub-models of their own (like Regression trees) and make predictions by cleverly training and aggregating the predictions of these sub-models. In this section, we build weather prediction models based on Gradient Boosting and Random Forests - two popular Ensemble methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees are an important class of Ensemble methods that trains a sequence of dependent Regression Trees. More specifically, Gradient Boosting trains Regression Trees sequentially, where the next Regression Tree is trained to correct the mistakes of the previously trained Tree. The Figure below captures this idea visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gradientboosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important note is that Gradient Boosting can only be performed when the response variable is a scalar. Since we need to predict a vector of length 300 (i.e the min, avg, and max temperatures for each of the next 5 days for each of the 20 stations), we will need to train one GradientBoosted Tree for each entry in this vector. That is, we trained 300 different Gradient Boosted Trees one corresponding to each measurement, station, and day combination.  The model class below implements this idea by using the MultiOutputRegression model from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=20,))\n",
    "window_size = 3\n",
    "grad_boosting = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we let each Gradient Boosting model use 20 regression Trees. Each Gradient Boosted tree was trained on the same covariates as the Regression-based Models. Like for the Regression-based models, we can again reuse the MetaPredictor model class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to Gradient Boosted Trees, we also implemented a prediction model using Random Forest. Similar to Gradient Boosting, Random Forest is a Tree-based Ensemble model. However, unlike Gradient Boosting, Random Forests use Bagging to aggregate the prediction of its Regression Trees. The figure below visualizes a Random Forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](random-forest-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, while Gradient Boosting can be used to make predictions when the response variable is a scalar, Random Forests can be trained and used for tasks when the response variable is a vector. To this end, we trained one Random Forest for each station, each of which outputs 15 predictions corresponding to the minimum, average, and maximum temperatures for the next 5 days. Thus, only a total of 20 Random Forest were trained. Each Random Forest used 100 Regression Trees of depth 5. In addition, the same covariates as in the Regression models and Gradient Boosting were also used for each of the Random Forest Models. The model class below formalizes this idea in code. Note that like the Regression-based models and Gradient Boosting, we can also use the Meta-Predictor here by passing in a Random Forest regression model into the constructor of the Meta-Predictor model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(max_depth=5)\n",
    "window_size = 3\n",
    "random_forest = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a model for deployment, we evaluated each of the models specified previously on a ten day evaluation window from Nov 30 - Dec 10 for the years of 2017, 2018, 2019, 2020, and 2021. More specifically, we simulated the exact same prediction process for 10 days between Nov 30 - Dec 10th across 5 different years. For each year and each model, we compute the average Mean Squared Error of that model's predictions for each of the 10 days. That is, for a given model, year, and day, the mean squared error was computed between the 300 temperature measurements predicted by the model and the true 300 temperature measurements corresponding to the min, avg, and max temperatures across all 20 stations for the next 5 days. This means, that for one particular year, each model had 10 MSE values, one per day in that year. The code below computes these MSEs for each model across all the years. The MSE value for the PreviousDayBaseline model are printed as an example of what the output looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scraped data in: 0.08515214920043945 s\n",
      "INFO:root:Scraped data in: 0.08239316940307617 s\n",
      "INFO:root:Scraped data in: 0.08758306503295898 s\n",
      "INFO:root:Scraped data in: 0.07847309112548828 s\n",
      "INFO:root:Scraped data in: 0.0869140625 s\n",
      "INFO:root:Scraped data in: 0.07189774513244629 s\n",
      "INFO:root:Scraped data in: 0.09340023994445801 s\n",
      "INFO:root:Scraped data in: 0.08368682861328125 s\n",
      "INFO:root:Scraped data in: 0.08802914619445801 s\n",
      "INFO:root:Scraped data in: 0.08450889587402344 s\n",
      "INFO:root:Scraped data in: 0.08315110206604004 s\n",
      "INFO:root:Scraped data in: 0.07562494277954102 s\n",
      "INFO:root:Scraped data in: 0.06784415245056152 s\n",
      "INFO:root:Scraped data in: 0.09171104431152344 s\n",
      "INFO:root:Scraped data in: 0.08259201049804688 s\n",
      "INFO:root:Scraped data in: 0.0841531753540039 s\n",
      "INFO:root:Scraped data in: 0.09810209274291992 s\n",
      "INFO:root:Scraped data in: 0.08394694328308105 s\n",
      "INFO:root:Scraped data in: 0.08850884437561035 s\n",
      "INFO:root:Scraped data in: 0.08823704719543457 s\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ARIMA' object has no attribute 'forecast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dx/4m0wl9l15hd_fqxpwpzslbb00000gs/T/ipykernel_68577/2790318046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_mses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel_mses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_mses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ARIMA\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dx/4m0wl9l15hd_fqxpwpzslbb00000gs/T/ipykernel_68577/944816998.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstart_eval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{year}-11-30\"\u001b[0m \u001b[0;31m# when eval period starts (must follow %Y-%m-%d format)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstart_eval_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_eval_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmses_per_year\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_single_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_eval_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwunderground_lookback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmses_per_year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dx/4m0wl9l15hd_fqxpwpzslbb00000gs/T/ipykernel_68577/944816998.py\u001b[0m in \u001b[0;36meval_single_window\u001b[0;34m(start_eval_date, eval_len, wunderground_lookback, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_eval_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_eval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dx/4m0wl9l15hd_fqxpwpzslbb00000gs/T/ipykernel_68577/2310681721.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmax_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mmin_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mavg_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmax_fc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmax_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ARIMA' object has no attribute 'forecast'"
     ]
    }
   ],
   "source": [
    "models = {\"PrevDay\":prev_day, \"Historic\":historic, \n",
    "\"WA\":WA_pred, \"ARIMA\":arima, \"OLS\":ols, \"LASSO\":lasso, \n",
    "\"Ridge\":ridge, \"MLP\":mlp, \"GB\":grad_boosting, \"RF\":random_forest}\n",
    "models = {\"ARIMA\":arima}\n",
    "\n",
    "model_mses = {}\n",
    "for model_name, model in models.items():\n",
    "    model_mses[model_name] = eval(model)\n",
    "\n",
    "print(model_mses[\"ARIMA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each model and each year, we compute an average over the 10 MSEs to produce a single average MSE representing the performance of that model in that year. The code below is function that takes as input the dictionary of MSE values for a particular model, and outputs the average MSE per year for that model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_mse(mses):\n",
    "    avg_mse = {}\n",
    "    for year, mse_list in mses.items():\n",
    "        avg_mse[year] = np.mean(mse_list)\n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function above across the MSE dictionaries for each model to compute an average MSE for each model in each of the 5 years. The results have been summarized in the table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2017: 113.1581879640379, 2018: 81.39315497797512, 2019: 97.44291151472791, 2020: 99.51453825096834, 2021: 115.48872792280513}\n"
     ]
    }
   ],
   "source": [
    "avg_mses = {}\n",
    "for model_name, mse_list in model_mses.items():\n",
    "    avg_mses[model_name] = compute_avg_mse(mse_list)\n",
    "\n",
    "print(avg_mses[\"PrevDay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model  | 2017  | 2018  | 2019  | 2020   | 2021   |\n",
    "|---|---|---|---|---|---|\n",
    "|Prev. Day | 113.16 | 81.39  | 97.44  | 99.51   | 115.49   |\n",
    "|Historic | 248.80 | 191.88  | 179 | 123   | 176   |\n",
    "|Weighted Avg. |133.27 | 87.54  | 104.96  | 80.22 | 113.01   |\n",
    "|ARIMA|  168.76| 92.86 | 108.16   | 124.9   | 146.57 |\n",
    "|OLS |   108  | 65.7  | 66.9  | 56.6   | 85.2 |\n",
    "|LASSO|  100.1  | 67.6  | 65.8   | 52.17   | 84.69 |\n",
    "|Ridge | 104.5 |65.2 | 67.1  | 54.1  | 84.3   |\n",
    "|MLP |  111.15 | 80.32 |67.14 |61.17 |105.18\n",
    "|Gradient Boosting |  101.08 | 77.38 | 54.30 |60.84 |85.81\n",
    "|Random Forest |  92.36 | 64.87 | 55.34 | 52.71 |83.97\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Table above, we find that the Random Forest model consistently outperfoms other models across all 5 years. This led us to deploy it in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although weather is caused by the atmospheric change that can, in principle, be described by laws of physics, we found that statistically modeling of the weather can be rather challenging. One key challenge is including all the  atmospheric factors that determines the weather in our statistical model. Nevertheless, we found that the weather can be predicted with a reasonable accuracy even with a few macro factors using regression-based and ensemble models. With right set of features and large training data, it may be possible to predict temperature fairly accurately by just using highly expressive statistical models. Finally, combining statistical models with physics-based constraints is an exciting future direction for weather prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('stats600')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5006b349036d30543356b78fdf2704715770f6187963a6b258d813bb4374e3b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
