{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "if \"data\" not in os.listdir(\".\"):\n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # ignore FutureWarnings from pd\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from raw_data import wunderground_download\n",
    "import predictor.utils as utils\n",
    "from predictor.models.predictor_zeros import ZerosPredictor\n",
    "from predictor.models.unique import ArimaPredictor\n",
    "from predictor.models.unique import HistoricAveragePredictor\n",
    "from predictor.models.seamus import BasicOLSPredictor\n",
    "from predictor.models.seamus import LassoPredictor\n",
    "from predictor.models.seamus import GBTPredictor\n",
    "from predictor.models.vinod import PrevDayHistoricalPredictor\n",
    "from predictor.models.vinod import MetaPredictor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from predictor.models.predictor_scaffold import Predictor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "keep_features = ['temp_min', 'wspd_min', 'pressure_min', 'heat_index_min', 'dewPt_min',\n",
    "   'temp_mean', 'wspd_mean', 'pressure_mean', 'heat_index_mean',\n",
    "   'dewPt_mean', 'temp_max', 'wspd_max', 'pressure_max', 'heat_index_max',\n",
    "   'dewPt_max', 'wdir_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_full_eval_data(start_eval_date, eval_len, wunderground_lookback):\n",
    "    \"\"\"Prepares data for evaluation for a window of [start_eval_date, start_eval_date + eval_len] \n",
    "    where eval_len is to be specified as the number of days. Note that the data returned from this\n",
    "    is NOT the data that is to be used for evaluation, i.e. each eval_day must be separated after\n",
    "    this initial bulk fetch (using get_eval_task)\n",
    "    \n",
    "    args:\n",
    "        start_eval_date: (datetime.datetime) day of first *evaluation*, i.e. first day where predictions are *made*\n",
    "            Note: that EACH eval day is evaluated for 5 days forward!\n",
    "        eval_len: how many eval days to include\n",
    "        wunderground_lookback: how far (in days) *before the first eval day* to extend the Wunderground data\n",
    "            Note: data scraping will take time proportional to this number\n",
    "    \"\"\"\n",
    "    noaa = utils.load_processed_data_src(\"noaa\")\n",
    "    full_eval_data = {}\n",
    "    for station in utils.stations:\n",
    "        full_eval_data[station] = {}\n",
    "        full_eval_data[station][\"noaa\"] = noaa[station]\n",
    "        full_eval_data[station][\"wunderground\"] = wunderground_download.fetch_wunderground_pd(\n",
    "            station, start_eval_date, eval_len, wunderground_lookback)\n",
    "    return full_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_eval_task(full_eval_data, prediction_date, station):\n",
    "    full_noaa = full_eval_data[station][\"noaa\"]\n",
    "    full_wunderground = full_eval_data[station][\"wunderground\"]\n",
    "\n",
    "    est = pytz.timezone('US/Eastern')\n",
    "    strict_cutoff = est.localize(prediction_date.replace(hour=12)) # all the predictions are going to be made noon EST\n",
    "\n",
    "    local_timezone = pytz.timezone(utils.fetch_timezone(station))\n",
    "    full_wunderground['date_col'] = pd.to_datetime(full_wunderground.index).tz_convert(local_timezone).date\n",
    "    \n",
    "    # cutoff_side = 0: < \"prediction cutoff\" -- used to construct our dataset\n",
    "    # cutoff_side = 1: > \"prediction cutoff\" -- used to construct the evaluation target\n",
    "    for cutoff_side in range(2):\n",
    "        if cutoff_side == 0:\n",
    "            dataset_view = full_wunderground[full_wunderground.index < strict_cutoff]\n",
    "        else:\n",
    "            dataset_view = full_wunderground[full_wunderground.index >= strict_cutoff]\n",
    "\n",
    "        # Wunderground returns granular (hourly) data points, but we only want daily for prediction: this coarsens the dataset\n",
    "        # TODO: time permitting, could remove this since it is a bit of a duplicate from process_wunderground\n",
    "        aggregated_columns = [\"temp\", \"wspd\", \"pressure\", \"heat_index\", 'dewPt']\n",
    "        maxes = dataset_view.groupby(['date_col'], sort=False)[aggregated_columns].max().set_axis([f\"{column}_max\" for column in aggregated_columns], axis=1, inplace=False).set_index(dataset_view['date_col'].unique())\n",
    "        means = dataset_view.groupby(['date_col'], sort=False)[aggregated_columns].mean().set_axis([f\"{column}_mean\" for column in aggregated_columns], axis=1, inplace=False).set_index(dataset_view['date_col'].unique())\n",
    "        mins  = dataset_view.groupby(['date_col'], sort=False)[aggregated_columns].min().set_axis([f\"{column}_min\" for column in aggregated_columns], axis=1, inplace=False).set_index(dataset_view['date_col'].unique())\n",
    "        wind_dir = dataset_view.groupby(['date_col'], sort=False)['wdir_cardinal'].agg(\n",
    "            lambda x: pd.Series.mode(x)[0]).astype(\"category\").to_frame(\"wdir_mode\").set_index(dataset_view['date_col'].unique())\n",
    "        aggregated_wunderground = pd.concat((mins, means, maxes, wind_dir), axis=1)\n",
    "\n",
    "        if cutoff_side == 0:\n",
    "            cut_wunderground = aggregated_wunderground.drop(aggregated_wunderground.index[0], axis=0) # first row is often partial day based on the time zone\n",
    "        else:\n",
    "            evaluation_data = aggregated_wunderground\n",
    "\n",
    "    noaa_cutoff_len = 3\n",
    "    noaa_cutoff = prediction_date - datetime.timedelta(days=noaa_cutoff_len)\n",
    "    cut_noaa = full_noaa.iloc[full_noaa.index < noaa_cutoff]\n",
    "    \n",
    "    forecast_horizon = 5\n",
    "    prediction_window = [prediction_date + datetime.timedelta(days=forecast_day) for forecast_day in range(1, forecast_horizon + 1)]\n",
    "    prediction_targets_df = evaluation_data.loc[prediction_window]\n",
    "    target = []\n",
    "    for i in range(len(prediction_targets_df)):\n",
    "        target.append(prediction_targets_df[\"temp_min\"][i])\n",
    "        target.append(prediction_targets_df[\"temp_mean\"][i])\n",
    "        target.append(prediction_targets_df[\"temp_max\"][i])\n",
    "    target = np.array(target)\n",
    "\n",
    "    data = {\n",
    "        \"noaa\": cut_noaa,\n",
    "        \"wunderground\": cut_wunderground,\n",
    "    }\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_task(full_eval_data, prediction_date):\n",
    "    full_data = {}\n",
    "    full_target = []\n",
    "    for station in utils.stations:\n",
    "        data, target = get_station_eval_task(full_eval_data, prediction_date, station)\n",
    "        full_data[station] = data\n",
    "        full_target.append(target.flatten())\n",
    "    full_target = np.array(full_target).flatten()\n",
    "    return full_data, full_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_single_window(start_eval_date, eval_len, wunderground_lookback, model):\n",
    "    \"\"\"Runs an evaluation for a window of [start_eval_date, start_eval_date + eval_len] \n",
    "    where eval_len is to be specified as the number of days\n",
    "    \n",
    "    args:\n",
    "        start_eval_date: (datetime.datetime) day of first *evaluation*, i.e. first day where predictions are *made*\n",
    "            Note: that EACH eval day is evaluated for 5 days forward!\n",
    "        eval_len: how many eval days to include\n",
    "        wunderground_lookback: how far (in days) *before the first eval day* to extend the Wunderground data\n",
    "            Note: data scraping will take time proportional to this number\n",
    "    \"\"\"\n",
    "    full_eval_data = prepare_full_eval_data(start_eval_date, eval_len, wunderground_lookback)\n",
    "    \n",
    "    mses = []\n",
    "    for day_offset in range(eval_len):\n",
    "        prediction_date = start_eval_date + datetime.timedelta(days=day_offset)\n",
    "        eval_data, eval_target = get_eval_task(full_eval_data, prediction_date)\n",
    "        \n",
    "        predictions = model.predict(eval_data)\n",
    "        mse = (np.square(eval_target - predictions)).mean()\n",
    "        print(mse)\n",
    "        mses.append(mse)\n",
    "    return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    \"\"\"Runs evaluations for a windows from 11/30 - 12/10 for multiple years (default: 10 years) \n",
    "    using the specified model as the predictor. Returns MSEs as a 20 x 15 matrix, with each station a row\n",
    "    across the 10 years with the year as the key of a dict, i.e.:\n",
    "    \n",
    "    {\n",
    "        2012: [MSEs],\n",
    "        2013: [MSEs],\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    start_year = 2019\n",
    "    num_years = 1\n",
    "    mses_per_year = {}\n",
    "    wunderground_lookback = 365 # how many days back to return of wunderground data\n",
    "    eval_len = 10 # how many days we running evaluation for\n",
    "    \n",
    "    for year in range(start_year, start_year + num_years):\n",
    "        start_eval_str = f\"{year}-11-30\" # when eval period starts (must follow %Y-%m-%d format)\n",
    "        start_eval_date = datetime.datetime.strptime(start_eval_str, \"%Y-%m-%d\") \n",
    "        mses_per_year[year] = eval_single_window(start_eval_date, eval_len, wunderground_lookback, model)\n",
    "    return mses_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Prediction\n",
    "By Yash Patel, Vinod Raman, Seamus Somerstep, and Unique Subedi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are tasked with predicting the minimum, average, and maximum temperature for the next five days for 20 different weather stations. This report summarizes our data pipeline and the models we used to tackle this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we discuss simple, computationally-efficient baseline models for predicting the minimum, average, and maximum temperatures for the next five days. These simple models enable us to better evaluate the performance of the more sophisticated models we tried. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Day Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first baseline model, termed the Previous Day Predictor, exploits the idea that for every station, today's minimum, average, and maximum tempertures are good estimates for the minimum, average, and maximum temperates for *each* of the next five days. To this end, the Previous Day Predictor predicts for each station, for each of the next five days, today's minimum, average, and maximum temperture as the respective minimum, average, and maximum temperature for that day. That is, for each station, the Previous Day Predictor outputs a vector of 15 values consisting of today's minimum, average, and maximum temperatures repeated five times in a row. The model class below formalizes this idea in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevDayPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, data):\n",
    "        stations_data = [list(data[station][\"wunderground\"].iloc[-1][[\"temp_min\",\"temp_mean\",\"temp_max\"]].values) * 5 for station in utils.stations]\n",
    "        stations_data = np.array(stations_data).flatten()\n",
    "        return stations_data\n",
    "\n",
    "prev_day_pred = PrevDayPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Wunderground data is used to compute \"today's\" minimum, average, and maximum temperature since the NOAA dataset lags by a couple days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical Average Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictor.models.predictor_scaffold import Predictor\n",
    "class HistoricAveragePredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        stations_data = []\n",
    "        for station in utils.stations:\n",
    "            noaa = data[station][\"noaa\"]\n",
    "            noaa = noaa.loc[:, [ \"TMIN\", \"TAVG\", \"TMAX\"]].dropna(axis =0)\n",
    "            noaa = noaa*0.18+32.0\n",
    "            current_date = noaa.index[-1]\n",
    "       \n",
    "            df = noaa.groupby(by=[noaa.index.month, noaa.index.day]).mean().round(2)\n",
    "            \n",
    "            for i in range(1, 6):\n",
    "                date_ = (current_date + pd.DateOffset(days=i))\n",
    "                stations_data.append(df[df.index == (date_.month, date_.day)].values.flatten())\n",
    "           \n",
    "       \n",
    "             \n",
    "           \n",
    "        stations_data = np.array(stations_data).flatten()\n",
    "        return stations_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to make a prediction for Dec 1. The rationale for this model is that the temperature of this year's Dec 1 will be similar to that of past Dec 1's. In particular, let $y_{i}$ be the maximum temperature of Dec 1 for year $i$. In NOAA dataset, we have $1941 \\leq i \\leq 2021$. Then, the prediction of maximum temperature of Dec 1, 2022 is \n",
    "$$\\frac{1}{2021-1941+1} \\sum_{i=1941}^{2021}y_i.$$\n",
    "So, our baseline is to predict historic average of minimum, average, and maximum temperatures for each station on that particular day. Unfortunately, this baseline did not end up performing that well for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weighted Average baseline predictor interpolates between the Previous Day Predictor and the Historical Average Predictor. At its core, it exploits the idea that tomorrow's temperature will be similar to today's temperature but the temperate 5 days from now will be more similar to the historical average temperature for that day. To this end, the Weighted Average Predictor computes a weighted average between today's temperature and the historical average temperature for each day and station. In other words, for each station, for each day, for each measurement (i.e., min, avg, max), the Weighted Average Predictor computes a weighted average between today's temperature and the historical average for the corresponding day. The model class below formalizes this idea in code. Note that it takes in as input a fixed set of mixing weights and uses the PreviousDayPredictor and the Historical Average Predictor as black-boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrevDayHistoricalPredictor(Predictor):\n",
    "    def __init__(self, weights):\n",
    "        self.prev_day_model = PrevDayPredictor()\n",
    "        self.historical_day_model = HistoricAveragePredictor()\n",
    "        self.weights = weights\n",
    "\n",
    "    def predict(self, data):\n",
    "        weights = self.weights\n",
    "\n",
    "        prev_day_pred = self.prev_day_model.predict(data)\n",
    "        historical_day_pred = self.historical_day_model.predict(data)\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        for index in range(len(prev_day_pred)//3):\n",
    "            prev_day_val = prev_day_pred[index*3:index*3 + 3]\n",
    "            historical_day_val= historical_day_pred[index*3:index*3 + 3]\n",
    "\n",
    "            prev_day_val_weighted = prev_day_val * weights[counter]\n",
    "            historical_day_val_weighted = historical_day_val * (1-weights[counter])\n",
    "\n",
    "            prev_day_pred[index*3:index*3 + 3] = prev_day_val_weighted\n",
    "            historical_day_pred[index*3:index*3 + 3] = historical_day_val_weighted\n",
    "\n",
    "            counter += 1\n",
    "            counter %= 5\n",
    "\n",
    "        predictions = (prev_day_pred + historical_day_pred).round(1)\n",
    "        return predictions\n",
    "\n",
    "WA_pred = PrevDayHistoricalPredictor(weights=[1, 0.80, 0.60, 0.40, 0.20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Weighted Average Predictor uses the same fixed set of 5 mixing weights between today's temperature and the historical average temperature across all stations and measurements.  That is, there is a single mixing weight associated to each day out.  The table below provides the mixing weights used in the instantiation WA_pred. \n",
    "\n",
    "| Days out | Weight on Today's temperature  | Weight on Historical Average temperature |\n",
    "|----------|--------------------------------|------------------------------------------|\n",
    "| 1        | 1                              | 0                                        |\n",
    "| 2        | 0.80                           | 0.20                                     |\n",
    "| 3        | 0.60                           | 0.40                                     |\n",
    "| 4        | 0.40                           | 0.60                                     |\n",
    "| 5        | 0.20                           | 0.80                                     |\n",
    "\n",
    "Observe that the weiht on Today's temperature decreases as the number of days out increases and vice versa for the weight on the historical average temperature. This coincides with the idea that tomorrow's temperature is most similar to today's temperature, but the temperature five days from now will be more similar to the historical average temperature for that day. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA, which stands for Auto-regressive Integrated Moving Average, are general models for time series forecasting. The ARIMA model for stationary time series are linear regression models and one set of its predictors are lags of the variables to be predicted.  A key concept in time series modeling is stationarity, which roughly says that the statistical properties of the time series is constant over time. The time series may not be stationary as it is, so we have to apply some linear/non-linear transformation to the data to bring stationarity. For example, differencing is a popular technique to bring stationarity. Suppose $Y_t$ be the maximum temperature at time stamp $t$. Then, the time series $\\{Y_t\\}_{t \\geq 1}$ may not be stationary, but the differenced variable $y_t = Y_{t} - Y_{t-1}$ perhaps is. Generally, we may have to use higher order of differencing. For example a second order differencing involves transforming time series to $y_{t} = (Y_{t} - Y_{t-1}) - (Y_{t-1} - Y_{t-2})$. On top of lagged difference of the variable, this model can also use the forecast errors in the past time steps, which are called moving average components. These errors denoted by $\\{\\epsilon_t\\}_{t \\geq 1}$ are assumped to to iid $\\text{Normal}(0,1)$. Therefore, an ARIMA model with $p$ auto-regressive component, $d$ order of differencing, and $q$ moving average components is \n",
    "$$y_{t} = \\mu + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} +\\ldots + \\beta_{p} y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\ldots + \\theta_{q} \\epsilon_{t-q},$$\n",
    "where $y_{t} = (Y_{t}- Y_{t-1}) - \\ldots - (Y_{t-d+1} - Y_{t-d}).$ We use ARIMA($p,d, q$) to denote this model.\n",
    "\n",
    "We trained three ARIMA models per station each for minimum, average, and maximum temperature. Thus, we have 60 ARIMA models all together for 20 stations. Each model forecast respective temperature for next five days. The model class below formalizes this idea in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "class ArimaPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, data):\n",
    "\n",
    "        stations_data = []\n",
    "        for station in utils.stations:\n",
    "\n",
    "            df = data[station][\"wunderground\"]\n",
    "            temp_max = df['temp_max'].asfreq('D')\n",
    "            temp_min = df['temp_min'].asfreq('D')\n",
    "            temp_avg = df['temp_mean'].asfreq('D')\n",
    "            \n",
    "\n",
    "            min_model = ARIMA(temp_min, order=(3,1,1))\n",
    "           \n",
    "            avg_model = ARIMA(temp_avg, order=(3,1,1))\n",
    "            max_model = ARIMA(temp_max, order=(3,1,1))\n",
    "            \n",
    "            min_fc = min_model.forecast(steps = 5)\n",
    "            avg_fc = avg_model.forecast(steps = 5)\n",
    "            max_fc  = max_model.forecast(steps = 5)\n",
    "            \n",
    "    \n",
    "            stations_data.append(np.vstack((min_fc.values, avg_fc.values, max_fc.values)).flatten())\n",
    "             \n",
    "           \n",
    "        stations_data = np.array(stations_data).flatten()\n",
    "        return stations_data\n",
    "\n",
    "arima = ArimaPredictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We obtained decent results with ARIMA model, but not in par with our regression based models. Moreover, we tried ARIMA models that takes into account various seasonal trends (monthly, yearly, weekly), but the model fitting took much longer with only marginal improvements in the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[add a brief statement]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for Regression-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Regression Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the features above, we created a Regression data set $(X, y)$. The $ith$ row of $(X, y)$ is associated with a particular date $i$ such that $X[i]$ is a 48-dimensional vector containing the aforementioned features for the previous  $3$ days ($3 * 16 = 48$ covariates in total) and $y[i]$ is a 15-dimensional vector containing the daily minimum, average, and maximum temperatures for the next $5$ days. As an example, if today's date was 10/11/2020, then the corresponding rows in $X$ and $y$ will contain the features for the days 10/8/2020 -10/10/2020 and the temperatures for the days 10/12/2020 - 10/16/2020. The function below uses Wunderground data for a given station to construct and output a regression dataset for that particular station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_data(data, window_size, features):\n",
    "    if features is not None:\n",
    "        keep_data = data[features]\n",
    "    else:\n",
    "        keep_data = data\n",
    "        \n",
    "    X, y = [], []\n",
    "    target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "    prediction_window = 5\n",
    "    for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "        X.append(keep_data.values[i:i+window_size,:-1].flatten())\n",
    "        y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "    test_X = keep_data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "    return np.array(X), np.array(y), test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ```data``` contains the Wunderground data for a specific station. The input ```window_size``` controls how many days of features should be included in each row of $X$, but this was fixed as $3$ (as mentioend above). The input ```features``` controls which features (i.e. columns of $X$) should be kept when constructing the regression dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_data(data, window_size, features):\n",
    "    if features is not None:\n",
    "        keep_data = data[features]\n",
    "    else:\n",
    "        keep_data = data\n",
    "        \n",
    "    X, y = [], []\n",
    "    target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "    prediction_window = 5\n",
    "    for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "        X.append(keep_data.values[i:i+window_size,:-1].flatten())\n",
    "        y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "    test_X = keep_data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "    return np.array(X), np.array(y), test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Meta-Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to expediate the process of model exploration, we additionally implemented a MetaPredictor class which takes in as input a blackbox implementation of a regression model, and uses it to make predictions. That is, instead of having to create a a new model class for every type of Regression model we want to try, we can now simply instantiate and pass any regression model as input into the constructor of the MetaPredictor. The MetaPredictor will then train 20 copies of the specified regression model on the regression dataset created by calling ```create_regression_data``` for each station, and then use each of the trained models to make a prediction of the temperatures for the next $5$ days.  The model class below formalizes this idea in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaPredictor(Predictor):\n",
    "    def __init__(self, reg, window_size, features= None):\n",
    "        self.reg = reg\n",
    "        self.window_size = window_size\n",
    "        self.features = features\n",
    "\n",
    "    def predict(self, data):\n",
    "        stations_data = []\n",
    "        start = time.time()\n",
    "        for station in utils.stations:\n",
    "            window_size = self.window_size\n",
    "            X, y, test_X = create_regression_data(data[station][\"wunderground\"], window_size, self.features) \n",
    "            self.reg.fit(X, y)\n",
    "            stations_data.append(self.reg.predict(test_X))\n",
    "        end = time.time()\n",
    "        logging.info(f\"Performed prediction in: {end - start} s\")\n",
    "        return np.array(stations_data).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each time the function predict is called, the MetaPredictor, makes 20 regression datasets, one for each station, trains the specified regression model on each of the datasets, and uses the trained models to make predictions for the next 5 days for each station. This process is done sequentially, one station at a time. Note that each time ```self.reg.fit``` is called, it erases its previous memory, so that a fresh new model is trained for each station. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few sub-sections, we show how the MetaPredictor model class can be used to create a variety of different regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression and ensemble based models the data source was strictly weather underground.  Data from this source is returned hourly so for feature preperation we aggregated the into daily data using the following calculations.  Average temperature is calculated as the average of the hourly temperatures while max/min temperature are calculated as the highest and lowest hourly temperature over the day respectively. Heat index, dew point, pressure and wind speed are aggregated in a similar manner.  Note that heat index is a human percieved temperature feature which combines heat with humidity.  The dew point is the temperature at which the relative humidity becomes 100%.  Finally wind direction was computed as the mode hourly wind direction from the possible categories of (CALM, S, SSE,SEE, E, NEE, NNE, N, NNW, NWW, W, SWW, SSW, VAR). Below is an aggregated table of the features used.\n",
    "\n",
    "| Features                     |\n",
    "|------------------------------|\n",
    "| Daily Min/Max/Avg Temp       |\n",
    "| Daily Min/Max/Avg Pressure   |\n",
    "| Daily Min/Max/Avg Heat Index |\n",
    "| Daily Min/Max/Avg Dew Point  |\n",
    "| Daily Min/Max/Avg Wind Speed |\n",
    "| Daily Mode Wind Direction    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three types of regression models were considered: OLS, Lasso and Ridge. Hyper-parameters were selected using the same cross validation technique discussed throughout the report. For Lasso we arrive at a regularization strength of  ð›¼=10  and for Ridge we arrive at a regularization strength of  ð›¼=100 . The class implementations of each of these models is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOLSPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def create_regression_data(self, data, window_size):\n",
    "        X, y = [], []\n",
    "        target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "        prediction_window = 5\n",
    "        for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "            X.append(data.values[i:i+window_size,:-1].flatten())\n",
    "            y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "        test_X = data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "        return np.array(X), np.array(y), test_X\n",
    "        \n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for station in utils.stations:\n",
    "            weather =  self.create_regression_data(data[station][\"wunderground\"], 3)\n",
    "            X = weather[0]\n",
    "            y = weather[1]\n",
    "            OLSfit = LinearRegression().fit(X,y)\n",
    "            Xprediction = weather[2]\n",
    "            pred = OLSfit.predict(Xprediction)\n",
    "            predictions.append(np.ravel(pred).tolist())\n",
    "        predictions = reduce(lambda x,y: x+y, predictions)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def create_regression_data(self, data, window_size):\n",
    "        X, y = [], []\n",
    "        target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "        prediction_window = 5\n",
    "        for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "            X.append(data.values[i:i+window_size,:-1].flatten())\n",
    "            y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "        test_X = data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "        return np.array(X), np.array(y), test_X\n",
    "        \n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for station in utils.stations:\n",
    "            weather =  self.create_regression_data(data[station][\"wunderground\"], 3)\n",
    "            X = weather[0]\n",
    "            y = weather[1]\n",
    "            Lassofit = Lasso(alpha = 10).fit(X,y)\n",
    "            Xprediction = weather[2]\n",
    "            pred = Lassofit.predict(Xprediction)\n",
    "            predictions.append(np.ravel(pred).tolist())\n",
    "        predictions = reduce(lambda x,y: x+y, predictions)\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgePredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def create_regression_data(self, data, window_size):\n",
    "        X, y = [], []\n",
    "        target_data = data[[\"temp_min\",\"temp_mean\",\"temp_max\"]].values\n",
    "        prediction_window = 5\n",
    "        for i in range(len(data) - (window_size + prediction_window + 1)):\n",
    "            X.append(data.values[i:i+window_size,:-1].flatten())\n",
    "            y.append(target_data[i+window_size+1:i+window_size+1+prediction_window].flatten())\n",
    "        test_X = data.values[-window_size-1:-1,:-1].flatten().reshape(1, -1) # the final frame used for future prediction\n",
    "        return np.array(X), np.array(y), test_X\n",
    "        \n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        for station in utils.stations:\n",
    "            weather =  self.create_regression_data(data[station][\"wunderground\"], 3)\n",
    "            X = weather[0]\n",
    "            y = weather[1]\n",
    "            Ridgefit = Ridge(alpha = 10).fit(X,y)\n",
    "            Xprediction = weather[2]\n",
    "            pred = Ridgefit.predict(Xprediction)\n",
    "            predictions.append(np.ravel(pred).tolist())\n",
    "        predictions = reduce(lambda x,y: x+y, predictions)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multilayer perceptron is a fully connected neural network. In particular, we used a two layer perceptron\n",
    "$$y  = W_2\\, \\sigma(\\, \\sigma(W_1x + b_1) +b_2 ), $$\n",
    "where the activation is relu. Each hidden layer is of dimension $20$ and the output layer has $15$ dimensions for min, average, and max of next five days. Since we trained one MLP model for one station, we trained 20 MLP regressors alltogether. The code below formalizes this idea by using the MLPRegressor class from sklearn and the aforementioned MetaPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor(random_state=1, hidden_layer_sizes=(20,20, ), max_iter=2000, solver='lbfgs', learning_rate= 'adaptive')\n",
    "window_size = 3\n",
    "mlp = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One main attraction of MLPs are that it allows for multitask regressions based on shared representations. It is particularly relevant here because the assumption of shared representation seems reasonable if we are predictive the response of same qualitative nature, temperature. We obtained pretty good results with this model, however, similar performance was obtained with ensemble methods with much less training time. So, we ended up choosing ensemble based models for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods like Boosting and Bagging are powerful prediction models for tabular data that are known to generalize in practice. These types of models contain sub-models of their own (like Regression trees) and make predictions by cleverly training and aggregating the predictions of these sub-models. In this section, we build weather prediction models based on Gradient Boosting and Random Forests - two popular Ensemble methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees are an important class of Ensemble methods that trains a sequence of dependent Regression Trees. More specifically, Gradient Boosting trains Regression Trees sequentially, where the next Regression Tree is trained to correct the mistakes of the previously trained Tree. The Figure below captures this idea visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gradientboosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important note is that Gradient Boosting can only be performed when the response variable is a scalar. Since we need to predict a vector of length 300 (i.e the min, avg, and max temperatures for each of the next 5 days for each of the 20 stations), we will need to train one GradientBoosted Tree for each entry in this vector. That is, we trained 300 different Gradient Boosted Trees one corresponding to each measurement, station, and day combination.  The model class below implements this idea by using the MultiOutputRegression model from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=20,))\n",
    "window_size = 3\n",
    "grad_boosting = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we let each Gradient Boosting model use 20 regression Trees. Each Gradient Boosted tree was trained on the same covariates as the Regression-based Models. Like for the Regression-based models, we can again reuse the MetaPredictor model class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to Gradient Boosted Trees, we also implemented a prediction model using Random Forest. Similar to Gradient Boosting, Random Forest is a Tree-based Ensemble model. However, unlike Gradient Boosting, Random Forests use Bagging to aggregate the prediction of its Regression Trees. The figure below visualizes a Random Forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](random-forest-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, while Gradient Boosting can be used to make predictions when the response variable is a scalar, Random Forests can be trained and used for tasks when the response variable is a vector. To this end, we trained one Random Forest for each station, each of which outputs 15 predictions corresponding to the minimum, average, and maximum temperatures for the next 5 days. Thus, only a total of 20 Random Forest were trained. Each Random Forest used 100 Regression Trees of depth 5. In addition, the same covariates as in the Regression models and Gradient Boosting were also used for each of the Random Forest Models. The model class below formalizes this idea in code. Note that like the Regression-based models and Gradient Boosting, we can also use the Meta-Predictor here by passing in a Random Forest regression model into the constructor of the Meta-Predictor model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(max_depth=5)\n",
    "window_size = 3\n",
    "random_forest = MetaPredictor(reg, window_size, keep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the validation MSE for each model for past five years. Random forest consistently outperfoms other models in each of the years, so we deployed it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model  | 2017  | 2018  | 2019  | 2020   | 2021   |\n",
    "|---|---|---|---|---|---|\n",
    "|Prev. Day | 113.16 | 81.39  | 97.44  | 99.51   | 115.49   |\n",
    "|Historic | 248.80 | 191.88  | 179 | 123   | 176   |\n",
    "|Weighted Avg. |133.27 | 87.54  | 104.96  | 80.22 | 113.01   |\n",
    "|ARIMA|  168.76| 92.86 | 108.16   | 124.9   | 146.57 |\n",
    "|OLS |   108  | 65.7  | 66.9  | 56.6   | 85.2 |\n",
    "|LASSO|  100.1  | 67.6  | 65.8   | 52.17   | 84.69 |\n",
    "|Ridge | 104.5 |65.2 | 67.1  | 54.1  | 84.3   |\n",
    "|MLP |  111.15 | 80.32 |67.14 |61.17 |105.18\n",
    "|Gradient Boosting |  101.08 | 77.38 | 54.30 |60.84 |85.81\n",
    "|Random Forest |  92.36 | 64.87 | 55.34 | 52.71 |83.97\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mses = eval(random_forest)\n",
    "print(eval_mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
